# -*- coding: utf-8 -*-
"""
First Machine Learning Project in Python Step-By-Step
    Installing the Python and SciPy platform.
    Loading the dataset.
    Summarizing the dataset.
    Visualizing the dataset.
    Evaluating some algorithms.
    Making some predictions.

There are 5 key libraries that you will need to install. 
    scipy
    numpy
    matplotlib
    pandas
    sklearn
    
# Check the versions of libraries
"""

# Check the versions of libraries
 
# Python version
import sys
print('Python: {}'.format(sys.version))
# scipy
import scipy
print('scipy: {}'.format(scipy.__version__))
# numpy
import numpy
print('numpy: {}'.format(numpy.__version__))
# matplotlib
import matplotlib
print('matplotlib: {}'.format(matplotlib.__version__))
# pandas
import pandas
print('pandas: {}'.format(pandas.__version__))
# scikit-learn
import sklearn
print('sklearn: {}'.format(sklearn.__version__))


#################################################

#Step#1  Load libraries
import pandas
from pandas.plotting import scatter_matrix
import matplotlib.pyplot as plt
from sklearn import model_selection
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

#Step#2  Load dataset
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/iris.csv"
names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']
dataset = pandas.read_csv(url, names=names)
 
#Step#3  Summarize the Dataset
#3.1.Dimensions of the dataset.               print(dataset.shape)
#3.2.Peek at the data itself.                 print(dataset.head(20)) 
#3.3.Statistical summary of all attributes.   print(dataset.describe())
#3.4.Breakdown of data by class variable.     print(dataset.groupby('class').size())
print(dataset.shape)                    #shape
print(dataset.head(20))                 #head
print(dataset.describe())               #descriptions
print(dataset.groupby('class').size())  #class distribution

#Step#4  Data Visualization
#4.1.Univariate Plots
# box and whisker plots
dataset.plot(kind='box', subplots=True, layout=(2,2), sharex=False, sharey=False)
plt.show()
# histograms
dataset.hist()
plt.show()

#4.2.Multivariate Plots
# scatter plot matrix
scatter_matrix(dataset)
plt.show()


#Step#5 Evaluate Some Algorithms
#5.1 Separate out a validation dataset.
#5.2 Set-up the test harness to use 10-fold cross validation.
#5.3 Build 5 different models to predict species from flower measurements
#5.4 Select the best model.

#5.1 Split-out TEST/Validation & training dataset
array = dataset.values
X = array[:,0:4]
Y = array[:,4]
validation_size = 0.20
seed = 7
X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)

#5.2 Test options and evaluation metric
seed = 7
scoring = 'accuracy'

#5.3 Build Models
"""
We don’t know which algorithms would be good on this problem or what configurations to use. We get an idea from the plots that some of the classes are partially linearly separable in some dimensions, so we are expecting generally good results.

Let’s evaluate 6 different algorithms:

Logistic Regression (LR)
Linear Discriminant Analysis (LDA)
K-Nearest Neighbors (KNN).
Classification and Regression Trees (CART).
Gaussian Naive Bayes (NB).
Support Vector Machines (SVM).

This is a good mixture of simple linear (LR and LDA), nonlinear (KNN, CART, NB and SVM) algorithms. We reset the random number seed before each run to ensure that the evaluation of each algorithm is performed using exactly the same data splits. It ensures the results are directly comparable.
"""
# Spot Check Algorithms
models = []
models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC(gamma='auto')))
# evaluate each model in turn
results = []
names = []
for name, model in models:
	kfold = model_selection.KFold(n_splits=10, random_state=seed)
	cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)
	results.append(cv_results)
	names.append(name)
	msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
	print(msg)


#5.4 Select Best Model
#We now have 6 models and accuracy estimations for each. We need to compare the models to each other and select the most accurate.
    
